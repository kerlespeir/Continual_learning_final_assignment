{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c8529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Training on classes: [0, 1, 2, 3, 4]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   7/ 64 (10.94%)\n",
      "  Layer conv3 | Dead:   0/128 (0.00%)\n",
      "  Layer fc1   | Dead:  26/128 (20.31%)\n",
      "  Layer fc2   | Dead:  37/128 (28.91%)\n",
      "Test Accuracy: 55.60% | Dead units: 14.583%\n",
      "\n",
      "==> Training on classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   1/ 64 (1.56%)\n",
      "  Layer conv3 | Dead:   0/128 (0.00%)\n",
      "  Layer fc1   | Dead:  16/128 (12.50%)\n",
      "  Layer fc2   | Dead:  33/128 (25.78%)\n",
      "Test Accuracy: 48.20% | Dead units: 10.417%\n",
      "\n",
      "==> Training on classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   1/ 64 (1.56%)\n",
      "  Layer conv3 | Dead:   0/128 (0.00%)\n",
      "  Layer fc1   | Dead:  15/128 (11.72%)\n",
      "  Layer fc2   | Dead:  18/128 (14.06%)\n",
      "Test Accuracy: 41.53% | Dead units: 7.083%\n",
      "\n",
      "==> Training on classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   1/ 64 (1.56%)\n",
      "  Layer conv3 | Dead:   0/128 (0.00%)\n",
      "  Layer fc1   | Dead:   9/128 (7.03%)\n",
      "  Layer fc2   | Dead:  16/128 (12.50%)\n",
      "Test Accuracy: 36.85% | Dead units: 5.417%\n",
      "\n",
      "==> Training on classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   1/ 64 (1.56%)\n",
      "  Layer conv3 | Dead:   0/128 (0.00%)\n",
      "  Layer fc1   | Dead:   9/128 (7.03%)\n",
      "  Layer fc2   | Dead:  11/128 (8.59%)\n",
      "Test Accuracy: 34.64% | Dead units: 4.375%\n",
      "\n",
      "==> Training on classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_incremental_loader(classes_seen, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m get_incremental_loader(classes_seen, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 154\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[1;32m    157\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[0;32mIn[13], line 113\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    114\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    115\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/functional.py:137\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_tensor\u001b[39m(pic) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    This function does not support torchscript.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    138\u001b[0m         _log_api_usage_once(to_tensor)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/jit/_trace.py:1115\u001b[0m, in \u001b[0;36mis_tracing\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_tracing\u001b[39m():\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m    Returns ``True`` in tracing (if a function is called during the tracing of\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    code with ``torch.jit.trace``) and ``False`` otherwise.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_is_tracing()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_jit_internal.py:1109\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m-> 1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m              return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 使用与论文相同的架构\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.last_filter_output = 2 * 2\n",
    "        self.num_conv_outputs = 128 * self.last_filter_output\n",
    "        self.fc1 = nn.Linear(self.num_conv_outputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            self.conv1, nn.ReLU(),\n",
    "            self.conv2, nn.ReLU(),\n",
    "            self.conv3, nn.ReLU(),\n",
    "            self.fc1, nn.ReLU(),\n",
    "            self.fc2, nn.ReLU(),\n",
    "            self.fc3\n",
    "        ])\n",
    "        self.act_type = 'relu'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predict(x)[0]\n",
    "\n",
    "    def predict(self, x):\n",
    "        x1 = self.pool(self.layers[1](self.layers[0](x)))\n",
    "        x2 = self.pool(self.layers[3](self.layers[2](x1)))\n",
    "        x3 = self.pool(self.layers[5](self.layers[4](x2)))\n",
    "        x3_flat = x3.view(-1, self.num_conv_outputs)\n",
    "        x4 = self.layers[7](self.layers[6](x3_flat))\n",
    "        x5 = self.layers[9](self.layers[8](x4))\n",
    "        x6 = self.layers[10](x5)\n",
    "        return x6, [x1, x2, x3_flat, x4, x5]\n",
    "\n",
    "    def count_dead_units(self, loader, device, threshold=1e-6):\n",
    "        self.eval()\n",
    "        layer_names = ['conv1', 'conv2', 'conv3', 'fc1', 'fc2']\n",
    "        dead_counts = {}\n",
    "        total_units = {}\n",
    "\n",
    "        # 初始化每层激活收集器\n",
    "        activations = {name: [] for name in layer_names}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, _ in loader:\n",
    "                x = x.to(device)\n",
    "                # 前向传播中获取中间层输出\n",
    "                x1 = self.pool(self.layers[1](self.layers[0](x)))  # conv1 + relu\n",
    "                x2 = self.pool(self.layers[3](self.layers[2](x1)))  # conv2 + relu\n",
    "                x3 = self.pool(self.layers[5](self.layers[4](x2)))  # conv3 + relu\n",
    "                x3f = x3.view(x3.size(0), -1)\n",
    "                x4 = self.layers[7](self.layers[6](x3f))  # fc1 + relu\n",
    "                x5 = self.layers[9](self.layers[8](x4))  # fc2 + relu\n",
    "\n",
    "                activations['conv1'].append(x1.cpu())\n",
    "                activations['conv2'].append(x2.cpu())\n",
    "                activations['conv3'].append(x3.cpu())\n",
    "                activations['fc1'].append(x4.cpu())\n",
    "                activations['fc2'].append(x5.cpu())\n",
    "\n",
    "        # 合并并统计\n",
    "        for name in layer_names:\n",
    "            layer_act = torch.cat(activations[name], dim=0)\n",
    "            layer_act_flat = layer_act.view(layer_act.shape[0], layer_act.shape[1], -1)  # [B, C, H*W]\n",
    "            layer_max = layer_act_flat.max(dim=2).values  # [B, C]\n",
    "            dead_mask = (layer_max.abs() < threshold).all(dim=0)  # 判断是否该神经元在整个 batch 中都为0\n",
    "            dead_count = dead_mask.sum().item()\n",
    "            total = layer_max.size(1)\n",
    "            dead_counts[name] = dead_count\n",
    "            total_units[name] = total\n",
    "\n",
    "        # 打印统计信息\n",
    "        print(\"Dead ReLU Unit Summary:\")\n",
    "        for name in layer_names:\n",
    "            pct = 100.0 * dead_counts[name] / total_units[name]\n",
    "            print(f\"  Layer {name:5s} | Dead: {dead_counts[name]:3d}/{total_units[name]:3d} ({pct:.2f}%)\")\n",
    "\n",
    "        # 总体百分比\n",
    "        total_dead = sum(dead_counts.values())\n",
    "        total_all = sum(total_units.values())\n",
    "        overall_pct = 100.0 * total_dead / total_all\n",
    "        return overall_pct\n",
    "\n",
    "\n",
    "\n",
    "# 载入数据\n",
    "def get_incremental_loader(classes, train=True, batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if label in classes]\n",
    "    subset = Subset(dataset, indices)\n",
    "    return torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=train)\n",
    "\n",
    "# 训练\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for epoch in range(200): \n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "#  训练代码\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ConvNet(num_classes=100).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "classes_seen = []\n",
    "accuracies = []\n",
    "dead_counts = []\n",
    "\n",
    "for i in range(0, 100, 5):\n",
    "    new_classes = list(range(i, i + 5))\n",
    "    classes_seen.extend(new_classes)\n",
    "\n",
    "    print(f\"\\n==> Training on classes: {classes_seen}\")\n",
    "    train_loader = get_incremental_loader(classes_seen, train=True)\n",
    "    test_loader = get_incremental_loader(classes_seen, train=False)\n",
    "\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    dead_pct = model.count_dead_units(test_loader, device)\n",
    "    dead_counts.append(dead_pct)\n",
    "\n",
    "    print(f\"Test Accuracy: {acc*100:.2f}% | Dead units: {dead_pct:.3f}%\")\n",
    "\n",
    "# 绘图\n",
    "steps = list(range(5, 105, 5))\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, accuracies, marker='o')\n",
    "plt.title('Accuracy vs #Classes')\n",
    "plt.xlabel('Number of Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, dead_counts, marker='o', color='r')\n",
    "plt.title('Dead ReLU Units vs #Classes')\n",
    "plt.xlabel('Number of Classes')\n",
    "plt.ylabel('Dead Units (Last Hidden Layer)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef36350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Task 1: classes [42, 41]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   1/128 (0.78%)\n",
      "  Layer fc1   | Dead:  14/128 (10.94%)\n",
      "  Layer fc2   | Dead:  35/128 (27.34%)\n",
      "Accuracy: 94.50% | Dead Unit %: 10.42%\n",
      "\n",
      "==> Task 2: classes [91, 9]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   1/128 (0.78%)\n",
      "  Layer fc1   | Dead:  11/128 (8.59%)\n",
      "  Layer fc2   | Dead:  35/128 (27.34%)\n",
      "Accuracy: 87.50% | Dead Unit %: 9.79%\n",
      "\n",
      "==> Task 3: classes [65, 50]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   0/128 (0.00%)\n",
      "  Layer fc1   | Dead:   9/128 (7.03%)\n",
      "  Layer fc2   | Dead:  31/128 (24.22%)\n",
      "Accuracy: 76.50% | Dead Unit %: 8.33%\n",
      "\n",
      "==> Task 4: classes [1, 70]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   3/128 (2.34%)\n",
      "  Layer fc1   | Dead:   7/128 (5.47%)\n",
      "  Layer fc2   | Dead:  31/128 (24.22%)\n",
      "Accuracy: 86.50% | Dead Unit %: 8.54%\n",
      "\n",
      "==> Task 5: classes [15, 78]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   3/128 (2.34%)\n",
      "  Layer fc1   | Dead:   9/128 (7.03%)\n",
      "  Layer fc2   | Dead:  34/128 (26.56%)\n",
      "Accuracy: 84.50% | Dead Unit %: 9.58%\n",
      "\n",
      "==> Task 6: classes [73, 10]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   8/128 (6.25%)\n",
      "  Layer fc1   | Dead:   9/128 (7.03%)\n",
      "  Layer fc2   | Dead:  28/128 (21.88%)\n",
      "Accuracy: 92.00% | Dead Unit %: 9.38%\n",
      "\n",
      "==> Task 7: classes [55, 56]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   1/ 64 (1.56%)\n",
      "  Layer conv3 | Dead:   9/128 (7.03%)\n",
      "  Layer fc1   | Dead:   6/128 (4.69%)\n",
      "  Layer fc2   | Dead:  31/128 (24.22%)\n",
      "Accuracy: 83.00% | Dead Unit %: 9.79%\n",
      "\n",
      "==> Task 8: classes [72, 45]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  11/128 (8.59%)\n",
      "  Layer fc1   | Dead:   8/128 (6.25%)\n",
      "  Layer fc2   | Dead:  24/128 (18.75%)\n",
      "Accuracy: 83.50% | Dead Unit %: 8.96%\n",
      "\n",
      "==> Task 9: classes [48, 92]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  14/128 (10.94%)\n",
      "  Layer fc1   | Dead:   5/128 (3.91%)\n",
      "  Layer fc2   | Dead:  25/128 (19.53%)\n",
      "Accuracy: 95.00% | Dead Unit %: 9.17%\n",
      "\n",
      "==> Task 10: classes [76, 37]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  21/128 (16.41%)\n",
      "  Layer fc1   | Dead:  15/128 (11.72%)\n",
      "  Layer fc2   | Dead:  24/128 (18.75%)\n",
      "Accuracy: 89.50% | Dead Unit %: 12.50%\n",
      "\n",
      "==> Task 11: classes [30, 21]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  40/128 (31.25%)\n",
      "  Layer fc1   | Dead:  23/128 (17.97%)\n",
      "  Layer fc2   | Dead:  30/128 (23.44%)\n",
      "Accuracy: 93.50% | Dead Unit %: 19.38%\n",
      "\n",
      "==> Task 12: classes [32, 96]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  13/128 (10.16%)\n",
      "  Layer fc1   | Dead:   5/128 (3.91%)\n",
      "  Layer fc2   | Dead:  26/128 (20.31%)\n",
      "Accuracy: 90.00% | Dead Unit %: 9.17%\n",
      "\n",
      "==> Task 13: classes [80, 49]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  13/128 (10.16%)\n",
      "  Layer fc1   | Dead:  14/128 (10.94%)\n",
      "  Layer fc2   | Dead:  41/128 (32.03%)\n",
      "Accuracy: 95.50% | Dead Unit %: 14.17%\n",
      "\n",
      "==> Task 14: classes [83, 26]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   9/128 (7.03%)\n",
      "  Layer fc1   | Dead:   3/128 (2.34%)\n",
      "  Layer fc2   | Dead:  27/128 (21.09%)\n",
      "Accuracy: 90.50% | Dead Unit %: 8.12%\n",
      "\n",
      "==> Task 15: classes [87, 33]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:   9/128 (7.03%)\n",
      "  Layer fc1   | Dead:   3/128 (2.34%)\n",
      "  Layer fc2   | Dead:  35/128 (27.34%)\n",
      "Accuracy: 90.50% | Dead Unit %: 9.79%\n",
      "\n",
      "==> Task 16: classes [8, 47]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  20/128 (15.62%)\n",
      "  Layer fc1   | Dead:   8/128 (6.25%)\n",
      "  Layer fc2   | Dead:  40/128 (31.25%)\n",
      "Accuracy: 94.50% | Dead Unit %: 14.17%\n",
      "\n",
      "==> Task 17: classes [59, 63]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  18/128 (14.06%)\n",
      "  Layer fc1   | Dead:   4/128 (3.12%)\n",
      "  Layer fc2   | Dead:  20/128 (15.62%)\n",
      "Accuracy: 90.00% | Dead Unit %: 8.75%\n",
      "\n",
      "==> Task 18: classes [74, 44]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dead ReLU Unit Summary:\n",
      "  Layer conv1 | Dead:   0/ 32 (0.00%)\n",
      "  Layer conv2 | Dead:   0/ 64 (0.00%)\n",
      "  Layer conv3 | Dead:  12/128 (9.38%)\n",
      "  Layer fc1   | Dead:   1/128 (0.78%)\n",
      "  Layer fc2   | Dead:  28/128 (21.88%)\n",
      "Accuracy: 77.00% | Dead Unit %: 8.54%\n",
      "\n",
      "==> Task 19: classes [98, 52]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# 模型定义\n",
    "# ----------------------------\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.last_filter_output = 2 * 2\n",
    "        self.num_conv_outputs = 128 * self.last_filter_output\n",
    "        self.fc1 = nn.Linear(self.num_conv_outputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            self.conv1, nn.ReLU(),\n",
    "            self.conv2, nn.ReLU(),\n",
    "            self.conv3, nn.ReLU(),\n",
    "            self.fc1, nn.ReLU(),\n",
    "            self.fc2, nn.ReLU(),\n",
    "            self.fc3\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predict(x)[0]\n",
    "\n",
    "    def predict(self, x):\n",
    "        x1 = self.pool(self.layers[1](self.layers[0](x)))\n",
    "        x2 = self.pool(self.layers[3](self.layers[2](x1)))\n",
    "        x3 = self.pool(self.layers[5](self.layers[4](x2)))\n",
    "        x3_flat = x3.view(x3.size(0), -1)\n",
    "        x4 = self.layers[7](self.layers[6](x3_flat))\n",
    "        x5 = self.layers[9](self.layers[8](x4))\n",
    "        x6 = self.layers[10](x5)\n",
    "        return x6, [x1, x2, x3, x4, x5]\n",
    "\n",
    "    def count_dead_units_all_layers(self, loader, device, threshold=1e-6):\n",
    "        self.eval()\n",
    "        layer_names = ['conv1', 'conv2', 'conv3', 'fc1', 'fc2']\n",
    "        dead_counts = {}\n",
    "        total_units = {}\n",
    "        activations = {name: [] for name in layer_names}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, _ in loader:\n",
    "                x = x.to(device)\n",
    "                _, acts = self.predict(x)\n",
    "                activations['conv1'].append(acts[0].cpu())\n",
    "                activations['conv2'].append(acts[1].cpu())\n",
    "                activations['conv3'].append(acts[2].cpu())\n",
    "                activations['fc1'].append(acts[3].cpu())\n",
    "                activations['fc2'].append(acts[4].cpu())\n",
    "\n",
    "        for name in layer_names:\n",
    "            layer_act = torch.cat(activations[name], dim=0)\n",
    "            layer_act_flat = layer_act.view(layer_act.shape[0], layer_act.shape[1], -1)\n",
    "            layer_max = layer_act_flat.max(dim=2).values\n",
    "            dead_mask = (layer_max.abs() < threshold).all(dim=0)\n",
    "            dead_count = dead_mask.sum().item()\n",
    "            total = layer_max.size(1)\n",
    "            dead_counts[name] = dead_count\n",
    "            total_units[name] = total\n",
    "\n",
    "        print(\"Dead ReLU Unit Summary:\")\n",
    "        for name in layer_names:\n",
    "            pct = 100.0 * dead_counts[name] / total_units[name]\n",
    "            print(f\"  Layer {name:5s} | Dead: {dead_counts[name]:3d}/{total_units[name]:3d} ({pct:.2f}%)\")\n",
    "\n",
    "        total_dead = sum(dead_counts.values())\n",
    "        total_all = sum(total_units.values())\n",
    "        overall_pct = 100.0 * total_dead / total_all\n",
    "        return overall_pct\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 数据加载函数\n",
    "# ----------------------------\n",
    "def get_loader_for_classes(classes, train=True, batch_size=90):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if label in classes]\n",
    "    subset = Subset(dataset, indices)\n",
    "    return torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=train)\n",
    "\n",
    "def get_incremental_loader(classes, train=True, batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if label in classes]\n",
    "    subset = Subset(dataset, indices)\n",
    "    return torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=train)\n",
    "# ----------------------------\n",
    "# 训练与评估\n",
    "# ----------------------------\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for epoch in range(200):  # 可改为10\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 主流程\n",
    "# ----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ConvNet(num_classes=100).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 生成不重复的任务\n",
    "all_classes = list(range(100))\n",
    "random.seed(42)\n",
    "random.shuffle(all_classes)\n",
    "task_classes = [all_classes[i:i + 2] for i in range(0, 100, 2)]\n",
    "\n",
    "accuracies = []\n",
    "dead_percents = []\n",
    "\n",
    "for task_id, class_group in enumerate(task_classes):\n",
    "    print(f\"\\n==> Task {task_id+1}: classes {class_group}\")\n",
    "    train_loader = get_loader_for_classes(class_group, train=True)\n",
    "    test_loader = get_loader_for_classes(class_group, train=False)\n",
    "    # train_loader = get_incremental_loader(classes_seen, train=True)\n",
    "    # test_loader = get_incremental_loader(classes_seen, train=False)\n",
    "\n",
    "\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "    dead_pct = model.count_dead_units_all_layers(test_loader, device)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    dead_percents.append(dead_pct)\n",
    "    print(f\"Accuracy: {acc*100:.2f}% | Dead Unit %: {dead_pct:.2f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# 可视化结果\n",
    "# ----------------------------\n",
    "steps = list(range(1, len(task_classes)+1))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, [a * 100 for a in accuracies], marker='o')\n",
    "plt.title('Accuracy vs Task ID')\n",
    "plt.xlabel('Task #')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, dead_percents, marker='o', color='red')\n",
    "plt.title('Dead ReLU Units vs Task ID')\n",
    "plt.xlabel('Task #')\n",
    "plt.ylabel('Dead Units (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
